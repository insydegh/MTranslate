15.09 Доделал флаер, диаграмму ганта, добавил список литературы в таблицах, сравнил нейросети

16.09: Добавил информацию о работе с аудио, добавил список библиотек для нейросети о обучении

17.09 нашел примеры нейросетей для распознавания речи на трех билиотеках

20.09 
проанализировано несколько кодов с гитхаба, множество безуспешных попыток загрузить на своем датасете
описан процесс изменения mp3 файлов в wav
а так же разложение речи по признакам (в нашем случае массив с числами)
переделана диаграмма ганта
21.09 - просмотрел процесс перевода текста в google translate api

23.09 начата работа над кодом, загружены данные для работы нейронки

24.09 попытки сформировать датасет, несколько ошибок о работе

25.09 начата работа над построением модели нейронной сети

26.09 создан словарь и перекодирована транскрипция (для каждой буквы - свой код)

27.09 - попробовал пропустить через нейросеть датасет из одного файла - получил ошибку, связанную с размерами input и output в слоях

       22:45 - разобрался с размерами, запустил модель для одного файла, прошло успешно, точность = 0
       
28.09 - подкорректировал транскрипции датасета, убрав из него лишние знаки, заменил цифры на их буквенные эквиваленты
собрать все данные в один массив все еще не получается из-за разных размеров данных

02.10 создан алгоритм программы
03.10 собран датасет с помощью словаря
04.10 протестирована модель на этих данных


" 1/1 [==============================] - 3s 3s/step - loss: 7666.3608 - accuracy: 0.0325 - val_loss: 7787.8711 - val_accuracy: 0.1302
Epoch 36/50
1/1 [==============================] - 3s 3s/step - loss: 7666.1924 - accuracy: 0.0371 - val_loss: 7787.6992 - val_accuracy: 0.1302
Epoch 37/50
1/1 [==============================] - 3s 3s/step - loss: 7666.0210 - accuracy: 0.0423 - val_loss: 7787.5269 - val_accuracy: 0.1302
Epoch 38/50
1/1 [==============================] - 3s 3s/step - loss: 7665.8525 - accuracy: 0.0302 - val_loss: 7787.3550 - val_accuracy: 0.1302
Epoch 39/50
1/1 [==============================] - 3s 3s/step - loss: 7665.6816 - accuracy: 0.0325 - val_loss: 7787.1836 - val_accuracy: 0.1302
Epoch 40/50
1/1 [==============================] - 4s 4s/step - loss: 7665.5151 - accuracy: 0.0354 - val_loss: 7787.0117 - val_accuracy: 0.1302
Epoch 41/50
1/1 [==============================] - 3s 3s/step - loss: 7665.3491 - accuracy: 0.0313 - val_loss: 7786.8364 - val_accuracy: 0.1302
Epoch 42/50
1/1 [==============================] - 3s 3s/step - loss: 7665.1724 - accuracy: 0.0389 - val_loss: 7786.6636 - val_accuracy: 0.1302
Epoch 43/50
"

model.predict(test)

array([[0.19097313, 0.11694564, 0.12770356, 0.14756751, 0.13873138,
        0.14608385, 0.13601106, 0.        , 0.10154705, 0.11037641,
        0.13054943, 0.15412618, 0.12790145, 0.13493891, 0.19392672,
        0.11485805, 0.1287998 , 0.12745285, 0.17703801, 0.15624249,
        0.2107229 , 0.10830312, 0.16898221, 0.08656514, 0.14481091,
        0.12264976, 0.15707083, 0.1455047 , 0.16423221, 0.05691147,
        0.15309821, 0.12346616, 0.1831852 , 0.18690133, 0.13688992,
        0.21421517, 0.10710988, 0.12541455, 0.21498455, 0.03034986,
        0.20200306, 0.11734976, 0.        , 0.14470744, 0.1625617 ,
        0.11550974, 0.1065028 , 0.08931633, 0.20159599, 0.07986582,
        0.11190031, 0.17847034, 0.147286  , 0.14110103, 0.13822788,
        0.1473004 , 0.106938  , 0.07980013, 0.13025706, 0.1279949 ,
        0.15302105, 0.15126266, 0.10379439, 0.13476446, 0.        ,
        0.12281401, 0.07958856, 0.14293808, 0.11508873, 0.08058199,
        0.21835136, 0.1100178 , 0.13541776, 0.03759566, 0.05940766,
        0.06706147, 0.08750885, 0.13025956, 0.0495368 , 0.12593614,
        0.04828899, 0.15336598, 0.10889217, 0.09030443, 0.14833562,
        0.16390565, 0.08467186, 0.13456778, 0.10322667, 0.0736603 ,
        0.06942935, 0.03125022, 0.11797382, 0.12401555, 0.06192996,
        0.03014426, 0.13403282, 0.05007719, 0.05476018, 0.07707909




5.10  посмотрел принцип работы word2vec

08.10 попытки преобразовать русский датасет с помощью word2vec - преобученные модели оказались неполными ( не считывали некоторые слова)
9.10 начало работы над английским датасетом, загрузка doc2vec модели для преобразования списка слов
11.10 обучение нейронной сети



